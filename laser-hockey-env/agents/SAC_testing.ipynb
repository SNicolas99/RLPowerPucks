{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "import random\n",
    "# import numpy.random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SAC and PPO class\n",
    "import os\n",
    "# goto parent directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from agents.SAC_advanced import SAC\n",
    "from agents.PPO import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defie ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "number of input neurons:  3\n",
      "tensor([1.8356], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9329], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6561], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7132], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7324], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.1618], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.3851], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.0411], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.8159], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9613], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.1946], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.2972], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8341], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8796], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.0485], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3525], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8560], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3777], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.0733], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5832], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.1094], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6446], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.2008], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9387], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3885], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.5311], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1764], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.5325], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4857], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7146], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3557], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9003], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7870], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.2120], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4150], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8485], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.1299], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.4767], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5631], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.6215], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1091], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9241], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.4738], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9144], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9282], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8550], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3515], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9665], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.9569], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9530], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.0872], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.0739], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5755], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.4047], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4052], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5105], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2391], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.2921], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6180], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6983], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.0795], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.5418], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3696], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6670], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.6178], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9753], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8255], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1682], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9055], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1716], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.2057], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1903], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.7513], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.2361], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5121], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8479], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.1179], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8078], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1383], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.5319], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5092], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.4917], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2925], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8913], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6895], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.2981], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3838], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3725], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.5497], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5463], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9701], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7053], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8499], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.0435], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6313], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9413], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.5597], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9916], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7242], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.3093], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.0577], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.6211], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9484], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3943], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6656], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.2513], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9608], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8729], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.2112], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.1222], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8524], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9135], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8250], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8107], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.4399], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.4915], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6667], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3622], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.9609], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.7115], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7167], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8290], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9404], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.6996], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.3069], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7947], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3484], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.2635], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8389], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7961], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4106], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9952], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2004], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.2116], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3582], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3548], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.0813], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3999], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.1592], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9348], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.3708], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.6974], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6177], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6428], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1181], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.3584], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.0905], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8953], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9482], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7872], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.3042], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.0565], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.3135], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8487], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4293], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7111], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.5854], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.8195], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.5645], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5396], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.6205], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.8192], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.0266], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.7155], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9913], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4475], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3266], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.1853], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9832], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.1407], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4477], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9599], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1445], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9121], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9932], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.6035], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.1474], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7538], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.4411], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9880], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9765], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4113], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.5446], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4943], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.0616], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.2780], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2338], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5693], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.7955], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1966], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.3980], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.9903], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.4246], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.9256], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9292], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.1602], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.2362], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8857], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.4705], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.6910], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1053], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9555], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3980], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6910], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.6923], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.3491], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.1135], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.3762], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2158], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.7734], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.9208], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.5562], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.0780], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8853], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8582], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.4680], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.7283], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8360], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.1455], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.6125], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8211], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.6743], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8408], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2976], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.8913], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3071], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.5747], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.4002], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.1176], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.3967], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.5830], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.4053], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.7262], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.5124], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7631], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.9329], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8988], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.7426], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9620], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9622], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.7271], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.9066], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.5079], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.4035], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2890], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.0731], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.2058], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.0657], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.4408], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.5244], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.9845], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.7966], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8111], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1.5717], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-1.8365], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([-0.8002], grad_fn=<ClampBackward1>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasschmitt/Desktop/RLPowerPucks/venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:173: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m# Update the agent\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(replay_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m256\u001b[39m:  \u001b[39m# Check if the replay buffer has enough samples\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     agent\u001b[39m.\u001b[39;49mupdate(replay_buffer, \u001b[39m256\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m \u001b[39m# Update the episode reward\u001b[39;00m\n\u001b[1;32m     65\u001b[0m episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Desktop/RLPowerPucks/laser-hockey-env/agents/SAC_advanced.py:112\u001b[0m, in \u001b[0;36mSAC.update\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m    109\u001b[0m     next_action \u001b[39m=\u001b[39m next_action\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    111\u001b[0m     \u001b[39m# Calculate the target value\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     target_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m next_log_prob \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(torch\u001b[39m.\u001b[39;49mcat([next_state, next_action], \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    113\u001b[0m     target_q \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m done) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m target_value\n\u001b[1;32m    115\u001b[0m \u001b[39m# Gradient descent step --> update the weights of the neural network\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "# Define the number of training steps\n",
    "num_steps = 10000\n",
    "\n",
    "# Create the Pendulum environment\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "# Create the SAC agent\n",
    "agent = SAC(env)\n",
    "\n",
    "# Create a replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=1000000)\n",
    "# replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "#    data_spec=agent.collect_data_spec,\n",
    "#    batch_size=env.batch_size,\n",
    "#    max_length=1000000)\n",
    "\n",
    "# Create a tensorboard writer for logging\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# start with training\n",
    "\n",
    "episode = 0\n",
    "step = 0\n",
    "\n",
    "# run the training loop\n",
    "while step < num_steps:\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    # Episode loop\n",
    "    while not done:\n",
    "        step += 1\n",
    "\n",
    "        # Choose an action# Choose an action\n",
    "        action, log_prob, _ = agent.get_action(torch.FloatTensor(state))\n",
    "\n",
    "        print(action)\n",
    "        # tensor([-1.4683], grad_fn=<ClampBackward1>)\n",
    "\n",
    "        print(type(action))\n",
    "        # <class 'torch.Tensor'>\n",
    "\n",
    "        # Convert the action tensor to a numpy array\n",
    "        with torch.no_grad():\n",
    "            action = action.numpy()\n",
    "\n",
    "        # Take a step in the environment\n",
    "        # next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update the agent\n",
    "        if len(replay_buffer) >= 256:  # Check if the replay buffer has enough samples\n",
    "            agent.update(replay_buffer, 256)\n",
    "\n",
    "        # Update the episode reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # Render the environment (optional)\n",
    "        env.render()\n",
    "\n",
    "        # Break if the maximum number of steps is reached\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "\n",
    "    # Log the episode reward\n",
    "    writer.add_scalar('Episode Reward', episode_reward, episode)\n",
    "\n",
    "    # Print the episode reward\n",
    "    print(f'Episode: {episode}\\tEpisode Reward: {episode_reward}')\n",
    "\n",
    "# Close the tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import time\\n# Initialize the episode and step counters\\nepisode = 0\\nstep = 0\\n\\n# Run the training loop\\nwhile step < num_steps:\\n    episode += 1\\n    episode_reward = 0\\n    done = False\\n\\n    # Reset the environment\\n    state = env.reset()\\n\\n    # Episode loop\\n    while not done:\\n        step += 1\\n\\n        # Choose an action# Choose an action\\n    action, log_prob, _ = agent.get_action(torch.FloatTensor(state))\\n\\n    # Convert the action tensor to a numpy array\\n    action = action.numpy()\\n\\n\\n        # Take a step in the environment\\n        next_state, reward, done, _ = env.step(action.detach().numpy())\\n\\n        # Store the transition in the replay buffer\\n        replay_buffer.add(state, action.detach().numpy(), reward, next_state, done)\\n\\n        # Update the agent\\n        agent.update(replay_buffer)\\n\\n        # Update the episode reward\\n        episode_reward += reward\\n\\n        # Update the state\\n        state = next_state\\n\\n        # Render the environment (optional)\\n        env.render()\\n\\n        # Break if the maximum number of steps is reached\\n        if step >= num_steps:\\n            break\\n\\n    # Log the episode reward\\n    writer.add_scalar('Episode Reward', episode_reward, episode)\\n\\n    # Print the episode reward\\n    print(f'Episode: {episode}\\tEpisode Reward: {episode_reward}')\\n\\n# Close the tensorboard writer\\nwriter.close()\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import time\n",
    "# Initialize the episode and step counters\n",
    "episode = 0\n",
    "step = 0\n",
    "\n",
    "# Run the training loop\n",
    "while step < num_steps:\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    # Episode loop\n",
    "    while not done:\n",
    "        step += 1\n",
    "\n",
    "        # Choose an action# Choose an action\n",
    "    action, log_prob, _ = agent.get_action(torch.FloatTensor(state))\n",
    "\n",
    "    # Convert the action tensor to a numpy array\n",
    "    action = action.numpy()\n",
    "\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.add(state, action.detach().numpy(), reward, next_state, done)\n",
    "\n",
    "        # Update the agent\n",
    "        agent.update(replay_buffer)\n",
    "\n",
    "        # Update the episode reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # Render the environment (optional)\n",
    "        env.render()\n",
    "\n",
    "        # Break if the maximum number of steps is reached\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "\n",
    "    # Log the episode reward\n",
    "    writer.add_scalar('Episode Reward', episode_reward, episode)\n",
    "\n",
    "    # Print the episode reward\n",
    "    print(f'Episode: {episode}\\tEpisode Reward: {episode_reward}')\n",
    "\n",
    "# Close the tensorboard writer\n",
    "writer.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Initialize the episode and step counters\\nepisode = 0\\nstep = 0\\n\\n# Run the training loop\\nwhile step < num_steps:\\n    episode += 1\\n    episode_reward = 0\\n    done = False\\n\\n    # Reset the environment\\n    state = env.reset()\\n\\n    # Episode loop\\n    while not done:\\n        step += 1\\n\\n        # Choose an action\\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Reshape state tensor\\n        action, log_prob, _ = agent.get_action(state_tensor)\\n        action = action.squeeze(0).detach().numpy()  # Convert to a numpy array\\n\\n        # Take a step in the environment\\n        next_state, reward, done, _ = env.step(action)\\n\\n        # Store the transition in the replay buffer\\n        replay_buffer.add(state, action, reward, next_state, done)\\n\\n        # Update the agent\\n        agent.update(replay_buffer)\\n\\n        # Update the episode reward\\n        episode_reward += reward\\n\\n        # Update the state\\n        state = next_state\\n\\n        # Render the environment (optional)\\n        env.render()\\n\\n        # Break if the maximum number of steps is reached\\n        if step >= num_steps:\\n            break\\n\\n    # Log the episode reward\\n    writer.add_scalar('Episode Reward', episode_reward, episode)\\n\\n    # Print the episode reward\\n    print(f'Episode: {episode}\\tEpisode Reward: {episode_reward}')\\n\\n# Close the tensorboard writer\\nwriter.close()\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Initialize the episode and step counters\n",
    "episode = 0\n",
    "step = 0\n",
    "\n",
    "# Run the training loop\n",
    "while step < num_steps:\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    # Episode loop\n",
    "    while not done:\n",
    "        step += 1\n",
    "\n",
    "        # Choose an action\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Reshape state tensor\n",
    "        action, log_prob, _ = agent.get_action(state_tensor)\n",
    "        action = action.squeeze(0).detach().numpy()  # Convert to a numpy array\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update the agent\n",
    "        agent.update(replay_buffer)\n",
    "\n",
    "        # Update the episode reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # Render the environment (optional)\n",
    "        env.render()\n",
    "\n",
    "        # Break if the maximum number of steps is reached\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "\n",
    "    # Log the episode reward\n",
    "    writer.add_scalar('Episode Reward', episode_reward, episode)\n",
    "\n",
    "    # Print the episode reward\n",
    "    print(f'Episode: {episode}\\tEpisode Reward: {episode_reward}')\n",
    "\n",
    "# Close the tensorboard writer\n",
    "writer.close()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Test the learned policy\\nstate = env.reset()[0]\\ndone = False\\ntotal_reward = 0\\n\\nwhile not done:\\n    action, _, _ = agent.get_action(torch.FloatTensor(state))\\n    next_state, reward, done, _, info = env.step(action.detach().numpy())\\n    total_reward += reward\\n    state = next_state\\n    env.render()\\n\\n# Print the total reward\\nprint(f'Test Total Reward: {total_reward}')\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Test the learned policy\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _, _ = agent.get_action(torch.FloatTensor(state))\n",
    "    next_state, reward, done, _, info = env.step(action.detach().numpy())\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    env.render()\n",
    "\n",
    "# Print the total reward\n",
    "print(f'Test Total Reward: {total_reward}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# go to parent folder\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from laserhockey.laser_hockey_env import LaserHockeyEnv\n",
    "from laserhockey.hockey_env import HockeyEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input neurons:  18\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x24 and 18x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# Perform a training update if the replay buffer has enough samples\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(replay_buffer\u001b[39m.\u001b[39mbuffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m256\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m         agent\u001b[39m.\u001b[39;49mupdate(replay_buffer, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m \u001b[39m# Store the episode reward\u001b[39;00m\n\u001b[1;32m     45\u001b[0m rewards\u001b[39m.\u001b[39mappend(episode_reward)\n",
      "File \u001b[0;32m~/Desktop/RLPowerPucks/laser-hockey-env/agents/SAC_advanced.py:107\u001b[0m, in \u001b[0;36mSAC.update\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():   \u001b[39m# no_grad() prevents calculating gradients for the target network\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     next_action, next_log_prob, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_action(next_state)\n\u001b[0;32m--> 107\u001b[0m     target_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m next_log_prob \u001b[39m-\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(torch\u001b[39m.\u001b[39;49mcat([next_state, next_action], \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     target_q \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m done) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m target_value\n\u001b[1;32m    110\u001b[0m \u001b[39m# Gradient descent step --> update the weights of the neural network\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/RLPowerPucks/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/RLPowerPucks/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/RLPowerPucks/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/RLPowerPucks/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x24 and 18x256)"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = LaserHockeyEnv()\n",
    "\n",
    "# Create the SAC agent\n",
    "agent = SAC(env=env)\n",
    "\n",
    "# Create the replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Lists to store the rewards and average rewards per episode\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "\n",
    "# Training loop\n",
    "total_steps = 0\n",
    "for episode in range(1, 10001):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action\n",
    "        action, _, _ = agent.get_action(torch.FloatTensor(state))\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, _, info = env.step(action.detach().numpy())\n",
    "\n",
    "        # Add the transition to the replay buffer\n",
    "        replay_buffer.add(state, action.detach().numpy(), reward, next_state, done)\n",
    "\n",
    "        # Accumulate the episode reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        # Increment the total steps\n",
    "        total_steps += 1\n",
    "\n",
    "        # Perform a training update if the replay buffer has enough samples\n",
    "        if len(replay_buffer.buffer) >= 256:\n",
    "            agent.update(replay_buffer, batch_size=256)\n",
    "\n",
    "    # Store the episode reward\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "    # Calculate the average reward over the last 100 episodes\n",
    "    avg_reward = np.mean(rewards[-100:])\n",
    "    avg_rewards.append(avg_reward)\n",
    "\n",
    "    # Print the episode information\n",
    "    print(f\"Episode {episode}: Reward = {episode_reward}, Average Reward = {avg_reward}\")\n",
    "\n",
    "    # Visualize the results\n",
    "    if episode % 100 == 0:\n",
    "        plt.plot(avg_rewards)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Reward\")\n",
    "        plt.title(\"Average Reward vs. Episode\")\n",
    "        plt.show()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
