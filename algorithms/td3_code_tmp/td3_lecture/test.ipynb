{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "from tools.trainer import Trainer\n",
    "from tools.logger import Logger\n",
    "from environments.hockey_wrapper import HockeyWrapper\n",
    "from TD3 import TD3Agent\n",
    "import laserhockey.hockey_env as h_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_string = \"Pendulum-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_string == \"Hockey\":\n",
    "    env = HockeyWrapper(mode=\"normal\", opponent=\"mixed\", add_opponents=True)\n",
    "else:\n",
    "    env = gym.make(env_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TD3Agent(env.observation_space, env.action_space)\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = Logger(2000, 100)\n",
    "# logger.load('logs/pendulum_tau_0_0004.npy')\n",
    "# logger.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player = h_env.BasicOpponent(weak=False)\n",
    "player = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(\"checkpoint_hockey.pth\")\n",
    "# agent.restore_state(state)\n",
    "# logger = Logger(30000, 100)\n",
    "# logger.load(\"logs/hockey_strong.npy\")\n",
    "# trainer.logger = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gekeleda/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/core.py:297: UserWarning: \u001b[33mWARN: env.ishockey to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.ishockey` for environment variables or `env.get_attr('ishockey')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/gekeleda/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/core.py:297: UserWarning: \u001b[33mWARN: env.add_opponents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.add_opponents` for environment variables or `env.get_attr('add_opponents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 101/2000:\n",
      "                    test reward: -1790.36\n",
      "                    mean reward: -1552.36, max reward: -907.96\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.061s\n",
      "                    Avg. critic loss: 0.16, Avg. actor loss: 8.84\n",
      "                    Avg. train duration: 0.275s\n",
      "                    \n",
      "            \n",
      "Step 201/2000:\n",
      "                    test reward: -1796.03\n",
      "                    mean reward: -1628.06, max reward: -770.04\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.058s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: 11.37\n",
      "                    Avg. train duration: 0.260s\n",
      "                    \n",
      "            \n",
      "Step 301/2000:\n",
      "                    test reward: -1467.30\n",
      "                    mean reward: -1616.11, max reward: -709.84\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.056s\n",
      "                    Avg. critic loss: 0.09, Avg. actor loss: 16.85\n",
      "                    Avg. train duration: 0.260s\n",
      "                    \n",
      "            \n",
      "Step 401/2000:\n",
      "                    test reward: -1391.03\n",
      "                    mean reward: -1473.80, max reward: -1060.20\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.060s\n",
      "                    Avg. critic loss: 0.11, Avg. actor loss: 22.30\n",
      "                    Avg. train duration: 0.287s\n",
      "                    \n",
      "            \n",
      "Step 501/2000:\n",
      "                    test reward: -1270.72\n",
      "                    mean reward: -1309.76, max reward: -796.88\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.058s\n",
      "                    Avg. critic loss: 0.12, Avg. actor loss: 26.87\n",
      "                    Avg. train duration: 0.275s\n",
      "                    \n",
      "            \n",
      "Step 601/2000:\n",
      "                    test reward: -1193.67\n",
      "                    mean reward: -1263.53, max reward: -859.41\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.055s\n",
      "                    Avg. critic loss: 0.15, Avg. actor loss: 30.80\n",
      "                    Avg. train duration: 0.260s\n",
      "                    \n",
      "            \n",
      "Step 701/2000:\n",
      "                    test reward: -1115.39\n",
      "                    mean reward: -1184.79, max reward: -12.86\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.055s\n",
      "                    Avg. critic loss: 0.16, Avg. actor loss: 34.43\n",
      "                    Avg. train duration: 0.251s\n",
      "                    \n",
      "            \n",
      "Step 801/2000:\n",
      "                    test reward: -1035.22\n",
      "                    mean reward: -1092.62, max reward: -1.21\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.055s\n",
      "                    Avg. critic loss: 0.19, Avg. actor loss: 37.74\n",
      "                    Avg. train duration: 0.254s\n",
      "                    \n",
      "            \n",
      "Step 901/2000:\n",
      "                    test reward: -1027.73\n",
      "                    mean reward: -1010.81, max reward: -10.68\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.058s\n",
      "                    Avg. critic loss: 0.20, Avg. actor loss: 40.82\n",
      "                    Avg. train duration: 0.268s\n",
      "                    \n",
      "            \n",
      "Step 1001/2000:\n",
      "                    test reward: -842.40\n",
      "                    mean reward: -893.04, max reward: -2.93\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.063s\n",
      "                    Avg. critic loss: 0.22, Avg. actor loss: 43.64\n",
      "                    Avg. train duration: 0.298s\n",
      "                    \n",
      "            \n",
      "Step 1101/2000:\n",
      "                    test reward: -557.05\n",
      "                    mean reward: -699.04, max reward: -0.61\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.058s\n",
      "                    Avg. critic loss: 0.24, Avg. actor loss: 45.90\n",
      "                    Avg. train duration: 0.270s\n",
      "                    \n",
      "            \n",
      "Step 1201/2000:\n",
      "                    test reward: -419.28\n",
      "                    mean reward: -608.55, max reward: -0.31\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.057s\n",
      "                    Avg. critic loss: 0.25, Avg. actor loss: 47.54\n",
      "                    Avg. train duration: 0.266s\n",
      "                    \n",
      "            \n",
      "Step 1301/2000:\n",
      "                    test reward: -379.95\n",
      "                    mean reward: -451.95, max reward: -0.68\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.056s\n",
      "                    Avg. critic loss: 0.26, Avg. actor loss: 48.63\n",
      "                    Avg. train duration: 0.249s\n",
      "                    \n",
      "            \n",
      "Step 1401/2000:\n",
      "                    test reward: -253.56\n",
      "                    mean reward: -524.69, max reward: -0.60\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.055s\n",
      "                    Avg. critic loss: 0.25, Avg. actor loss: 49.55\n",
      "                    Avg. train duration: 0.248s\n",
      "                    \n",
      "            \n",
      "Step 1501/2000:\n",
      "                    test reward: -208.12\n",
      "                    mean reward: -476.77, max reward: -0.33\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.056s\n",
      "                    Avg. critic loss: 0.27, Avg. actor loss: 50.39\n",
      "                    Avg. train duration: 0.258s\n",
      "                    \n",
      "            \n",
      "Step 1601/2000:\n",
      "                    test reward: -194.07\n",
      "                    mean reward: -466.72, max reward: -0.57\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.055s\n",
      "                    Avg. critic loss: 0.28, Avg. actor loss: 51.13\n",
      "                    Avg. train duration: 0.256s\n",
      "                    \n",
      "            \n",
      "Step 1701/2000:\n",
      "                    test reward: -192.41\n",
      "                    mean reward: -314.45, max reward: -0.46\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.057s\n",
      "                    Avg. critic loss: 0.27, Avg. actor loss: 51.89\n",
      "                    Avg. train duration: 0.260s\n",
      "                    \n",
      "            \n",
      "Step 1801/2000:\n",
      "                    test reward: -197.25\n",
      "                    mean reward: -626.13, max reward: -0.13\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.056s\n",
      "                    Avg. critic loss: 0.29, Avg. actor loss: 52.68\n",
      "                    Avg. train duration: 0.254s\n",
      "                    \n",
      "            \n",
      "Step 1901/2000:\n",
      "                    test reward: -169.70\n",
      "                    mean reward: -348.02, max reward: -0.23\n",
      "                    Avg. step count: 200.0, Avg ep duration: 0.057s\n",
      "                    Avg. critic loss: 0.31, Avg. actor loss: 53.40\n",
      "                    Avg. train duration: 0.267s\n",
      "                    \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "ep_rewards = trainer.train(env, agent, n_episodes=2000, test_every=100, noise=0.2, player=player, mixed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'winrate' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mlogger\u001b[39m.\u001b[39;49mplot()\n",
      "File \u001b[0;32m~/RLPowerPucks/algorithms/td3_code_tmp/td3_lecture/../../../tools/logger.py:86\u001b[0m, in \u001b[0;36mLogger.plot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m     lossrate \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlossrate)\n\u001b[1;32m     85\u001b[0m test_episodes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(test_rewards\u001b[39m.\u001b[39msize) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_interval\n\u001b[0;32m---> 86\u001b[0m hockey_episodes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(winrate\u001b[39m.\u001b[39msize) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_interval\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m test_rewards\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNo test rewards to plot\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'winrate' referenced before assignment"
     ]
    }
   ],
   "source": [
    "trainer.logger.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env_string, agent, n_episodes=10, noise=0, opponent='strong', render=None):\n",
    "    if env_string == 'Hockey':\n",
    "        env = HockeyWrapper(mode='normal', opponent=opponent)\n",
    "        if render is None:\n",
    "            render = True\n",
    "    else:\n",
    "        env = gym.make(env_string, render_mode='human')\n",
    "        render = False\n",
    "    # player = h_env.BasicOpponent(weak=False)\n",
    "    player = agent\n",
    "    steps, rewards, observations, actions, results = Trainer.run(env, player, n_episodes=n_episodes, noise=noise, render=render, store_transitions=False, hockey=env_string=='Hockey')\n",
    "    env.close()\n",
    "    return steps, rewards, observations, actions, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_opp = h_env.BasicOpponent(weak=False)\n",
    "# test_opp = env.opponent_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-128.3945349910451\n"
     ]
    }
   ],
   "source": [
    "steps, rewards, observations, actions, results = play(env_string, agent, n_episodes=10, noise=0, render=True, opponent=test_opp)\n",
    "print(np.mean(rewards))\n",
    "# print(np.mean(results==1), np.mean(results==0), np.mean(results==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m steps, rewards, observations, actions, results \u001b[39m=\u001b[39m play(env_string, agent, n_episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, noise\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, opponent\u001b[39m=\u001b[39;49mtest_opp)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(rewards))\n\u001b[1;32m      3\u001b[0m \u001b[39m# print(np.mean(results==1), np.mean(results==0), np.mean(results==-1))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(env_string, agent, n_episodes, noise, opponent, render)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# player = h_env.BasicOpponent(weak=False)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m player \u001b[39m=\u001b[39m agent\n\u001b[0;32m---> 11\u001b[0m steps, rewards, observations, actions, results \u001b[39m=\u001b[39m Trainer\u001b[39m.\u001b[39;49mrun(env, player, n_episodes\u001b[39m=\u001b[39;49mn_episodes, noise\u001b[39m=\u001b[39;49mnoise, render\u001b[39m=\u001b[39;49mrender, store_transitions\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, hockey\u001b[39m=\u001b[39;49menv_string\u001b[39m==\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mHockey\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m steps, rewards, observations, actions, results\n",
      "File \u001b[0;32m~/RLPowerPucks/algorithms/td3_code_tmp/td3_lecture/../../../tools/trainer.py:45\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(env, player, n_episodes, noise, store_transitions, render, hockey)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     action \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39mact(state, noise)\n\u001b[0;32m---> 45\u001b[0m (next_state, reward, done, _trunc, _info) \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m render:\n\u001b[1;32m     47\u001b[0m     env\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/envs/classic_control/pendulum.py:143\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([newth, newthdot])\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs(), \u001b[39m-\u001b[39mcosts, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/envs/classic_control/pendulum.py:259\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    258\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    260\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps, rewards, observations, actions, results = play(env_string, agent, n_episodes=100, noise=0, render=False, opponent=test_opp)\n",
    "print(np.mean(rewards))\n",
    "# print(np.mean(results==1), np.mean(results==0), np.mean(results==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger.save(\"logs/pendulum_tau_0_0004.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state(), 'checkpoint_pendulum_tau_0_0004.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_state = torch.load('checkpoint_hockey.pth')\n",
    "# agent.restore_state(agent_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
