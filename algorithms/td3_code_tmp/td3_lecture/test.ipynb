{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from DDPG import DDPGAgent\n",
    "\n",
    "from utils.trainer import Trainer\n",
    "from utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_string = \"Pendulum-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 101/30000:\n",
      "                    test reward: -1307.71\n",
      "                    mean reward: -1410.63, max reward: -771.76\n",
      "                    Avg. step number: 199.0, Avg ep duration: 0.014s\n",
      "                    Avg. train duration: 0.195s\n",
      "            \n",
      "Step 201/30000:\n",
      "                    test reward: -201.83\n",
      "                    mean reward: -1044.83, max reward: -138.09\n",
      "                    Avg. step number: 199.0, Avg ep duration: 0.014s\n",
      "                    Avg. train duration: 0.195s\n",
      "            \n",
      "Step 301/30000:\n",
      "                    test reward: -174.26\n",
      "                    mean reward: -827.65, max reward: -1.19\n",
      "                    Avg. step number: 199.0, Avg ep duration: 0.014s\n",
      "                    Avg. train duration: 0.192s\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "ep_rewards = trainer.train(env, agent, n_episodes=30000, test_every=100, noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.logger.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env_string, agent, n_episodes=10, noise=0):\n",
    "    env = gym.make(env_string, render_mode='human')\n",
    "    steps, rewards, observations, actions = Trainer.run(env, agent, n_episodes=n_episodes, noise=noise)\n",
    "    env.close()\n",
    "    return steps, rewards, observations, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m steps, rewards, observations, actions \u001b[39m=\u001b[39m play(\u001b[39m\"\u001b[39;49m\u001b[39mPendulum-v1\u001b[39;49m\u001b[39m\"\u001b[39;49m, agent, n_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, noise\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39msum(rewards))\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(env_string, agent, n_episodes, noise)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay\u001b[39m(env_string, agent, n_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, noise\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m      2\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(env_string, render_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     steps, rewards, observations, actions \u001b[39m=\u001b[39m Trainer\u001b[39m.\u001b[39;49mrun(env, agent, n_episodes\u001b[39m=\u001b[39;49mn_episodes, noise\u001b[39m=\u001b[39;49mnoise)\n\u001b[1;32m      4\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m steps, rewards, observations, actions\n",
      "File \u001b[0;32m~/bwSyncAndShare/stud/mast/Sem2/RL/project/RLPowerPucks/algorithms/td3_code_tmp/td3_lecture/../../../utils/trainer.py:18\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(env, agent, n_episodes, noise, store_transitions)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2000\u001b[39m):\n\u001b[1;32m     17\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state, noise)\n\u001b[0;32m---> 18\u001b[0m     (next_state, reward, done, _trunc, _info) \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     20\u001b[0m     done \u001b[39m=\u001b[39m _trunc \u001b[39mor\u001b[39;00m done\n\u001b[1;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m store_transitions:\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/envs/classic_control/pendulum.py:143\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([newth, newthdot])\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs(), \u001b[39m-\u001b[39mcosts, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/mambaforge/envs/rl/lib/python3.8/site-packages/gymnasium/envs/classic_control/pendulum.py:259\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    258\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    260\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps, rewards, observations, actions = play(\"Pendulum-v1\", agent, n_episodes=1, noise=0)\n",
    "print(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger.save(\"pendulum_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.state(), 'checkpoint_pendulum.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_state = torch.load('checkpoint_pendulum.pth')\n",
    "# agent.restore_state(agent_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
