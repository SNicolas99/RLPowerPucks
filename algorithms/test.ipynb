{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from tools.trainer import Trainer\n",
    "from tools.logger import Logger\n",
    "from environments.hockey_wrapper import HockeyWrapper\n",
    "from TD3 import TD3Agent\n",
    "import laserhockey.hockey_env as h_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_string = \"Hockey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_string == \"Hockey\":\n",
    "    env = HockeyWrapper(mode=\"normal\", opponent=\"weak\", add_opponents=False)\n",
    "else:\n",
    "    env = gym.make(env_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TD3Agent(env.observation_space, env.action_space)\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = Logger(10000, 100)\n",
    "# logger.hockey = True\n",
    "# logger.load('logs/hockey_strong_mixed_new.npy')\n",
    "# logger.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = h_env.BasicOpponent(weak=False)\n",
    "# player = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(\"checkpoint_hockey_add_long.pth\")\n",
    "# agent.restore_state(state)\n",
    "# logger = Logger(30000, 100)\n",
    "# logger.load(\"logs/hockey_strong.npy\")\n",
    "# trainer.logger = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/50000:\n",
      "                    test reward: -13.88\n",
      "                    mean reward: 0.50, max reward: 11.98\n",
      "                    Avg. step count: 177.4, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: 0.12\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.10, drawrate: 0.39, lossrate: 0.52\n",
      "            \n",
      "Step 200/50000:\n",
      "                    test reward: -12.25\n",
      "                    mean reward: 2.00, max reward: 12.28\n",
      "                    Avg. step count: 175.1, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: 0.10\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.11, drawrate: 0.34, lossrate: 0.55\n",
      "            \n",
      "Step 300/50000:\n",
      "                    test reward: -14.48\n",
      "                    mean reward: 2.19, max reward: 12.59\n",
      "                    Avg. step count: 167.6, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: 0.08\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.06, drawrate: 0.35, lossrate: 0.60\n",
      "            \n",
      "Step 400/50000:\n",
      "                    test reward: -12.32\n",
      "                    mean reward: 0.99, max reward: 12.92\n",
      "                    Avg. step count: 169.7, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: 0.06\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.07, drawrate: 0.41, lossrate: 0.52\n",
      "            \n",
      "Step 500/50000:\n",
      "                    test reward: -12.72\n",
      "                    mean reward: 0.29, max reward: 12.60\n",
      "                    Avg. step count: 162.1, Avg ep duration: 0.023s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: 0.03\n",
      "                    Avg. train duration: 0.070s\n",
      "                    winrate: 0.09, drawrate: 0.40, lossrate: 0.50\n",
      "            \n",
      "Step 600/50000:\n",
      "                    test reward: -16.48\n",
      "                    mean reward: 1.30, max reward: 12.19\n",
      "                    Avg. step count: 167.3, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.03\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.10, drawrate: 0.43, lossrate: 0.47\n",
      "            \n",
      "Step 700/50000:\n",
      "                    test reward: -18.42\n",
      "                    mean reward: 1.33, max reward: 12.19\n",
      "                    Avg. step count: 178.3, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.10\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.08, drawrate: 0.37, lossrate: 0.55\n",
      "            \n",
      "Step 800/50000:\n",
      "                    test reward: -20.26\n",
      "                    mean reward: 2.67, max reward: 12.46\n",
      "                    Avg. step count: 174.6, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.13\n",
      "                    Avg. train duration: 0.072s\n",
      "                    winrate: 0.07, drawrate: 0.43, lossrate: 0.50\n",
      "            \n",
      "Step 900/50000:\n",
      "                    test reward: -19.38\n",
      "                    mean reward: -0.11, max reward: 11.44\n",
      "                    Avg. step count: 177.6, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.15\n",
      "                    Avg. train duration: 0.070s\n",
      "                    winrate: 0.05, drawrate: 0.51, lossrate: 0.44\n",
      "            \n",
      "Step 1000/50000:\n",
      "                    test reward: -13.75\n",
      "                    mean reward: 1.02, max reward: 12.20\n",
      "                    Avg. step count: 175.6, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.17\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.06, drawrate: 0.42, lossrate: 0.52\n",
      "            \n",
      "Step 1100/50000:\n",
      "                    test reward: -20.24\n",
      "                    mean reward: 1.42, max reward: 12.70\n",
      "                    Avg. step count: 170.4, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.19\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.06, drawrate: 0.52, lossrate: 0.43\n",
      "            \n",
      "Step 1200/50000:\n",
      "                    test reward: -21.85\n",
      "                    mean reward: 2.07, max reward: 13.19\n",
      "                    Avg. step count: 177.8, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.21\n",
      "                    Avg. train duration: 0.071s\n",
      "                    winrate: 0.06, drawrate: 0.55, lossrate: 0.39\n",
      "            \n",
      "Step 1300/50000:\n",
      "                    test reward: -23.84\n",
      "                    mean reward: 0.49, max reward: 11.79\n",
      "                    Avg. step count: 179.5, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.03, Avg. actor loss: -0.23\n",
      "                    Avg. train duration: 0.071s\n",
      "                    winrate: 0.03, drawrate: 0.51, lossrate: 0.46\n",
      "            \n",
      "Step 1400/50000:\n",
      "                    test reward: -24.60\n",
      "                    mean reward: 1.67, max reward: 11.88\n",
      "                    Avg. step count: 176.8, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.04, Avg. actor loss: -0.25\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.02, drawrate: 0.54, lossrate: 0.44\n",
      "            \n",
      "Step 1500/50000:\n",
      "                    test reward: -26.33\n",
      "                    mean reward: 1.55, max reward: 13.03\n",
      "                    Avg. step count: 148.7, Avg ep duration: 0.021s\n",
      "                    Avg. critic loss: 0.04, Avg. actor loss: -0.28\n",
      "                    Avg. train duration: 0.067s\n",
      "                    winrate: 0.02, drawrate: 0.59, lossrate: 0.39\n",
      "            \n",
      "Step 1600/50000:\n",
      "                    test reward: -25.05\n",
      "                    mean reward: -0.45, max reward: 12.64\n",
      "                    Avg. step count: 173.6, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.04, Avg. actor loss: -0.31\n",
      "                    Avg. train duration: 0.067s\n",
      "                    winrate: 0.02, drawrate: 0.56, lossrate: 0.41\n",
      "            \n",
      "Step 1700/50000:\n",
      "                    test reward: -23.36\n",
      "                    mean reward: 0.69, max reward: 12.91\n",
      "                    Avg. step count: 179.7, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.05, Avg. actor loss: -0.35\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.04, drawrate: 0.53, lossrate: 0.42\n",
      "            \n",
      "Step 1800/50000:\n",
      "                    test reward: -24.44\n",
      "                    mean reward: 0.39, max reward: 11.85\n",
      "                    Avg. step count: 160.0, Avg ep duration: 0.023s\n",
      "                    Avg. critic loss: 0.05, Avg. actor loss: -0.38\n",
      "                    Avg. train duration: 0.070s\n",
      "                    winrate: 0.03, drawrate: 0.54, lossrate: 0.42\n",
      "            \n",
      "Step 1900/50000:\n",
      "                    test reward: -24.33\n",
      "                    mean reward: 0.36, max reward: 12.76\n",
      "                    Avg. step count: 182.8, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.05, Avg. actor loss: -0.41\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.03, drawrate: 0.56, lossrate: 0.41\n",
      "            \n",
      "Step 2000/50000:\n",
      "                    test reward: -24.51\n",
      "                    mean reward: 2.84, max reward: 12.79\n",
      "                    Avg. step count: 168.8, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.05, Avg. actor loss: -0.45\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.04, drawrate: 0.51, lossrate: 0.45\n",
      "            \n",
      "Step 2100/50000:\n",
      "                    test reward: -25.67\n",
      "                    mean reward: 0.94, max reward: 12.17\n",
      "                    Avg. step count: 170.6, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.05, Avg. actor loss: -0.49\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.03, drawrate: 0.54, lossrate: 0.44\n",
      "            \n",
      "Step 2200/50000:\n",
      "                    test reward: -24.05\n",
      "                    mean reward: -0.05, max reward: 11.62\n",
      "                    Avg. step count: 177.3, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.52\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.03, drawrate: 0.49, lossrate: 0.48\n",
      "            \n",
      "Step 2300/50000:\n",
      "                    test reward: -24.74\n",
      "                    mean reward: 1.11, max reward: 11.94\n",
      "                    Avg. step count: 171.9, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.56\n",
      "                    Avg. train duration: 0.072s\n",
      "                    winrate: 0.02, drawrate: 0.49, lossrate: 0.48\n",
      "            \n",
      "Step 2400/50000:\n",
      "                    test reward: -24.15\n",
      "                    mean reward: 0.28, max reward: 11.66\n",
      "                    Avg. step count: 177.9, Avg ep duration: 0.025s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.60\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.05, drawrate: 0.52, lossrate: 0.43\n",
      "            \n",
      "Step 2500/50000:\n",
      "                    test reward: -27.41\n",
      "                    mean reward: 1.45, max reward: 12.18\n",
      "                    Avg. step count: 167.3, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.64\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.03, drawrate: 0.57, lossrate: 0.40\n",
      "            \n",
      "Step 2600/50000:\n",
      "                    test reward: -25.61\n",
      "                    mean reward: 0.88, max reward: 13.10\n",
      "                    Avg. step count: 181.6, Avg ep duration: 0.026s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.69\n",
      "                    Avg. train duration: 0.068s\n",
      "                    winrate: 0.05, drawrate: 0.54, lossrate: 0.42\n",
      "            \n",
      "Step 2700/50000:\n",
      "                    test reward: -21.22\n",
      "                    mean reward: 3.12, max reward: 11.94\n",
      "                    Avg. step count: 157.2, Avg ep duration: 0.022s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.74\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.07, drawrate: 0.45, lossrate: 0.49\n",
      "            \n",
      "Step 2800/50000:\n",
      "                    test reward: -16.05\n",
      "                    mean reward: 2.23, max reward: 13.25\n",
      "                    Avg. step count: 171.9, Avg ep duration: 0.024s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.79\n",
      "                    Avg. train duration: 0.069s\n",
      "                    winrate: 0.13, drawrate: 0.36, lossrate: 0.51\n",
      "            \n",
      "Step 2900/50000:\n",
      "                    test reward: -22.58\n",
      "                    mean reward: 2.76, max reward: 13.84\n",
      "                    Avg. step count: 155.3, Avg ep duration: 0.022s\n",
      "                    Avg. critic loss: 0.06, Avg. actor loss: -0.85\n",
      "                    Avg. train duration: 0.070s\n",
      "                    winrate: 0.04, drawrate: 0.41, lossrate: 0.55\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "ep_rewards = trainer.train(env, agent, n_episodes=50000, test_every=100, n_test_episodes=1000, noise=0.05, player=player, mixed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.logger.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env_string, agent, n_episodes=10, noise=0, opponent='strong', render=None):\n",
    "    if env_string == 'Hockey':\n",
    "        env = HockeyWrapper(mode='normal', opponent=opponent)\n",
    "        if render is None:\n",
    "            render = True\n",
    "    else:\n",
    "        env = gym.make(env_string, render_mode='human')\n",
    "        render = False\n",
    "    # player = h_env.BasicOpponent(weak=False)\n",
    "    player = agent\n",
    "    steps, rewards, observations, actions, results = Trainer.run(env, player, n_episodes=n_episodes, noise=noise, render=render, store_transitions=False, hockey=env_string=='Hockey')\n",
    "    env.close()\n",
    "    return steps, rewards, observations, actions, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_opp = h_env.BasicOpponent(weak=True)\n",
    "# test_opp = env.opponent_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.822249979497756\n",
      "0.5 0.5 0.0\n"
     ]
    }
   ],
   "source": [
    "steps, rewards, observations, actions, results = play(env_string, agent, n_episodes=2, noise=0, render=True, opponent=test_opp)\n",
    "print(np.mean(rewards))\n",
    "print(np.mean(results==1), np.mean(results==0), np.mean(results==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps, rewards, observations, actions, results = play(env_string, agent, n_episodes=100, noise=0, render=False, opponent=test_opp)\n",
    "# print(np.mean(rewards))\n",
    "# # print(np.mean(results==1), np.mean(results==0), np.mean(results==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger.save(\"logs/hockey_comp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state(), 'checkpoint_hockey_comp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_state = torch.load('checkpoint_hockey.pth')\n",
    "# agent.restore_state(agent_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
